{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7942528,"sourceType":"datasetVersion","datasetId":4669790}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fair-esm==2.0.0","metadata":{"execution":{"iopub.status.busy":"2024-05-30T03:10:49.039677Z","iopub.execute_input":"2024-05-30T03:10:49.040148Z","iopub.status.idle":"2024-05-30T03:11:05.549422Z","shell.execute_reply.started":"2024-05-30T03:10:49.040115Z","shell.execute_reply":"2024-05-30T03:11:05.547794Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting fair-esm==2.0.0\n  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\nDownloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fair-esm\nSuccessfully installed fair-esm-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import esm\nimport torch\nimport collections\nimport pandas as pd\n\ndef esm_embeddings(peptide_sequence_list, model_name):\n    \n    model_dict = {\n        'esm2_t6_8M_UR50D': (esm.pretrained.esm2_t6_8M_UR50D, 6),\n        'esm2_t12_35M_UR50D': (esm.pretrained.esm2_t12_35M_UR50D, 12),\n        'esm2_t30_150M_UR50D': (esm.pretrained.esm2_t30_150M_UR50D, 30),\n        'esm2_t33_650M_UR50D': (esm.pretrained.esm2_t33_650M_UR50D, 33),\n    }\n    \n    # Check if the provided model name is valid or not\n    if model_name not in model_dict:\n        raise ValueError(f\"Invalid model name '{model_name}'. Please choose from {list(model_dict.keys())}.\")\n    \n    model_func, num_layers = model_dict[model_name]\n    model, alphabet = model_func()\n    \n    batch_converter = alphabet.get_batch_converter()\n    model.eval()  \n\n    # Load the peptide sequence list into the batch_converter\n    batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n\n    # Extract per-residue representations (on CPU)\n    with torch.no_grad():\n        results = model(batch_tokens, repr_layers=[num_layers], return_contacts=True)\n    \n    token_representations = results[\"representations\"][num_layers] \n    sequence_representations = []\n    for i, tokens_len in enumerate(batch_lens):\n        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n    \n    embeddings_results = collections.defaultdict(list)\n    for i in range(len(sequence_representations)):\n        # tensor can be transformed as numpy sequence_representations[i].numpy() or sequence_representations[i].tolist\n        each_seq_rep = sequence_representations[i].tolist()\n        for each_element in each_seq_rep:\n            embeddings_results[i].append(each_element)\n    \n    embeddings_results = pd.DataFrame(embeddings_results).T\n    return embeddings_results","metadata":{"execution":{"iopub.status.busy":"2024-05-30T03:22:20.883284Z","iopub.execute_input":"2024-05-30T03:22:20.885243Z","iopub.status.idle":"2024-05-30T03:22:20.902858Z","shell.execute_reply.started":"2024-05-30T03:22:20.885145Z","shell.execute_reply":"2024-05-30T03:22:20.901061Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Example usage\n\n# Add the dataset\ndataset = pd.read_excel('MRSA-25-3-2024.xlsx', na_filter = False) \nsequence_list = dataset['seq']\npeptide_sequence_list = []\n\n# Prepare sequence_list to esm process \nfor seq in sequence_list:\n    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n    tuple_sequence = tuple(format_seq)\n    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n\n# Usage esm_embedding and other esm models in these hashtags\n#embeddings_results = esm_embeddings(peptide_sequence_list,'esm2_t6_8M_UR50D')\n#embeddings_results = esm_embeddings(peptide_sequence_list,'esm2_t12_35M_UR50D')\n#embeddings_results = esm_embeddings(peptide_sequence_list,'esm2_t30_150M_UR50D')\nembeddings_results = esm_embeddings(peptide_sequence_list,'esm2_t33_650M_UR50D')\n\n# Convert to pandas\nembeddings_results = pd.DataFrame(embeddings_results)\nembeddings_results","metadata":{"execution":{"iopub.status.busy":"2024-05-30T03:23:45.389897Z","iopub.execute_input":"2024-05-30T03:23:45.390445Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\nDownloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n","output_type":"stream"}]}]}