{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7942528,"sourceType":"datasetVersion","datasetId":4669790}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import T5Tokenizer, T5EncoderModel\nimport re\nimport torch\nimport pandas as pd\n\ndef generate_protein_embeddings(sequence_list, model_name):\n    if model_name not in ['prot_t5_xl_bfd', 'prot_t5_xl_uniref50']:\n        raise ValueError(\"Invalid model name. Please choose either 'prot_t5_xl_bfd' or 'prot_t5_xl_uniref50'.\")\n\n    # Preprocess the sequence list\n    sequence_list = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequence_list]\n    sequence_lengths = [len(sequence) for sequence in sequence_list]\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    # Load the tokenizer and model\n    tokenizer = T5Tokenizer.from_pretrained(f'Rostlab/{model_name}', do_lower_case=False)\n    model = T5EncoderModel.from_pretrained(f'Rostlab/{model_name}').to(device)\n\n    # Set model precision based on the device\n    if device.type == 'cpu':\n        model.float()\n    else:\n        model.half()\n\n    # Tokenize sequences and pad them up to the longest sequence in the batch\n    sequence_list = [\" \".join(list(sequence)) for sequence in sequence_list]\n    ids = tokenizer(sequence_list, add_special_tokens=True, padding=\"longest\")\n    input_ids = torch.tensor(ids['input_ids']).to(device)\n    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n\n    # Generate embeddings\n    with torch.no_grad():\n        embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n\n    # Extract residue embeddings for each sequence in the batch and remove padded & special tokens\n    embeddings = [embedding_repr.last_hidden_state[i, :length] for i, length in enumerate(sequence_lengths)]\n\n    # Derive a single representation (per-protein embedding) for the whole protein\n    per_protein_embeddings = [emb.mean(dim=0) for emb in embeddings]\n\n    # Convert embeddings to a DataFrame\n    embeddings_df = pd.DataFrame([emb.cpu().numpy() for emb in per_protein_embeddings])\n    return embeddings_df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T03:44:46.799106Z","iopub.execute_input":"2024-05-30T03:44:46.799762Z","iopub.status.idle":"2024-05-30T03:44:58.396502Z","shell.execute_reply.started":"2024-05-30T03:44:46.799725Z","shell.execute_reply":"2024-05-30T03:44:58.395733Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_excel('MRSA-25-3-2024.xlsx', na_filter = False) \nsequence_list = dataset['seq']\npeptide_sequence_list = []\n\n# Example Usage by 'prot_t5_xl_uniref50' or 'prot_t5_xl_bfd' as the second parameter\nembeddings_df = generate_protein_embeddings(sequence_list, 'prot_t5_xl_uniref50') \n\n# Display protT5 feature\nembeddings_df ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T03:45:16.934097Z","iopub.execute_input":"2024-05-30T03:45:16.934629Z","iopub.status.idle":"2024-05-30T03:50:33.342464Z","shell.execute_reply.started":"2024-05-30T03:45:16.934598Z","shell.execute_reply":"2024-05-30T03:50:33.341610Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c6d4ffe0684dec8cf3a80af6a25a8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/238k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d32f9324edb442b28bc889a392489b8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b7689ff44e4c98a9c26cc41f32e406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3735828f3f7b4248ab18cc5e96acb03f"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/11.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99aacaf8bb7498fa8007040c3796ef1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]}]}